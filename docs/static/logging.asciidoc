[[logging]]
=== Logging

Logstash emits internal logs during its operation, which are placed in `LS_HOME/logs` (or `/var/log/logstash` for
DEB/RPM). The default logging level is `INFO`. Logstash's logging framework is based on
http://logging.apache.org/log4j/2.x/[Log4j 2 framework], and much of its functionality is exposed directly to users.

You can configure logging for a particular subsystem, module, or plugin.

When you need to debug problems, particularly problems with plugins, consider
increasing the logging level to `DEBUG` to get more verbose messages. For
example, if you are debugging issues with Elasticsearch Output, you can increase
log levels just for that component. This approach reduces noise from
excessive logging and helps you focus on the problem area.

You can configure logging using the `log4j2.properties` file or the Logstash API.

* *`log4j2.properties` file.*  Changes made through the `log4j2.properties`
file require you to restart Logstash for the changes to take effect.  Changes *persist*
through subsequent restarts. 
* *Logging API.* Changes made through the Logging API are effective immediately 
without a restart. The changes *do not persist* after Logstash
is restarted.

[[log4j2]]
==== Log4j2 configuration

Logstash ships with a `log4j2.properties` file with out-of-the-box settings, including logging to console. You
can modify this file to change the rotation policy, type, and other
https://logging.apache.org/log4j/2.x/manual/configuration.html#Loggers[log4j2
configuration]. 

You must restart Logstash to apply any changes that you make to
this file.
Changes to `log4j2.properties` persist after Logstash is restarted.

Here's an example using `outputs.elasticsearch`:

[source,yaml]
--------------------------------------------------
logger.elasticsearchoutput.name = logstash.outputs.elasticsearch
logger.elasticsearchoutput.level = debug
--------------------------------------------------

The previous example defines a name and level for the logger `logstash.outputs.elasticsearch`.
The logger is usually identified by a Java class name, such as
`org.logstash.dissect.Dissector`, for example.  It can also be a partial package
path as in `org.logstash.dissect`.  For Ruby classes, like `LogStash::Outputs::Elasticsearch`,
the logger name is obtained by lowercasing the full class name and replacing double colons with a single dot.

It's strongly recommended to use the default log4j configuration that's shipped with {ls}, however the
following section describe in detail how the rolling strategy works, so you have more control on these settings.

In `log4j2.properties` file are defined three appenders to write on log files: one for plain text, one with json format,
and another to split log lines on per pipeline basis when using the `pipeline.separate_logs` setting.
These appenders define triggering policies and rollover strategy. The triggering policy determines if a rollover
should be performed, while the strategy defines how the rollover should be done.
By default definition two triggering policies are used: time and size. The time policy creates one file per day
while the size policy forces the creation of a new file once the file size surpasses 100 MB.

The default strategy also performs file rollovers to a maximum number of 30 files and also deletes files older than seven days.
The rollover limit imposes that a maximum of 30 files is created. When the 31-st has to be created a rollover happen the first
is removed - all the other are renamed - so that 2nd becomes 1st and 30th becomes 29th, creating the space for the new file.
Each file will have a date and if the files are older than 7 days (by default) they are removed during rollover.

[source,text]
----------------------------------
appender.rolling.type = RollingFile <1>
appender.rolling.name = plain_rolling
appender.rolling.fileName = ${sys:ls.logs}/logstash-plain.log <2>
appender.rolling.filePattern = ${sys:ls.logs}/logstash-plain-%d{yyyy-MM-dd}-%i.log.gz <3>
appender.rolling.policies.type = Policies
appender.rolling.policies.time.type = TimeBasedTriggeringPolicy <4>
appender.rolling.policies.time.interval = 1
appender.rolling.policies.time.modulate = true
appender.rolling.layout.type = PatternLayout
appender.rolling.layout.pattern = [%d{ISO8601}][%-5p][%-25c]%notEmpty{[%X{pipeline.id}]}%notEmpty{[%X{plugin.id}]} %m%n
appender.rolling.policies.size.type = SizeBasedTriggeringPolicy <5>
appender.rolling.policies.size.size = 100MB
appender.rolling.strategy.type = DefaultRolloverStrategy
appender.rolling.strategy.max = 30 <6>
appender.rolling.strategy.action.type = Delete <7>
appender.rolling.strategy.action.basepath = ${sys:ls.logs}
appender.rolling.strategy.action.condition.type = IfFileName
appender.rolling.strategy.action.condition.glob = logstash-plain-* <8>
appender.rolling.strategy.action.condition.nested_condition.type = IfLastModified
appender.rolling.strategy.action.condition.nested_condition.age = 7D <9>
----------------------------------
<1> The appender type, which rolls older log files.
<2> Name of the current log file.
<3> Name's format definition of the rolled files, in this case a date followed by an incremental number, up to 30 (by default).
<4> Time policy to trigger a rollover at the end of the day.
<5> Size policy to trigger a rollover once the plain text file reaches the size of 100 MB.
<6> Rollover strategy defines a maximum of 30 files.
<7> Action to execute during the rollover.
<8> The file set to consider by the action.
<9> Condition to execute the rollover action: older than 7 days.

The rollover action can also enforce a disk usage limit, deleting older files to match
the requested condition, as an example:

[source,text]
----------------------------------
appender.rolling.type = RollingFile
...
appender.rolling.strategy.action.condition.glob = pipeline_${ctx:pipeline.id}.*.log.gz
appender.rolling.strategy.action.condition.nested_condition.type = IfAccumulatedFileSize
appender.rolling.strategy.action.condition.nested_condition.exceeds = 5MB <1>
----------------------------------
<1> Delete files if total accumulated compressed file size is over 5MB.

==== Logging APIs

For temporary logging changes, modifying the `log4j2.properties` file and restarting Logstash leads to unnecessary
downtime. Instead, you can dynamically update logging levels through the logging API. These settings are effective
immediately and do not need a restart. 

NOTE: By default, the logging API attempts to bind to `tcp:9600`. If this port is already in use by another Logstash
instance, you need to launch Logstash with the `--api.http.port` flag specified to bind to a different port. See
<<command-line-flags>> for more information.

===== Retrieve list of logging configurations

To retrieve a list of logging subsystems available at runtime, you can do a `GET` request to `_node/logging`

[source,js]
--------------------------------------------------
curl -XGET 'localhost:9600/_node/logging?pretty'
--------------------------------------------------

Example response:

["source","js"]
--------------------------------------------------
{
...
  "loggers" : {
    "logstash.agent" : "INFO",
    "logstash.api.service" : "INFO",
    "logstash.basepipeline" : "INFO",
    "logstash.codecs.plain" : "INFO",
    "logstash.codecs.rubydebug" : "INFO",
    "logstash.filters.grok" : "INFO",
    "logstash.inputs.beats" : "INFO",
    "logstash.instrument.periodicpoller.jvm" : "INFO",
    "logstash.instrument.periodicpoller.os" : "INFO",
    "logstash.instrument.periodicpoller.persistentqueue" : "INFO",
    "logstash.outputs.stdout" : "INFO",
    "logstash.pipeline" : "INFO",
    "logstash.plugins.registry" : "INFO",
    "logstash.runner" : "INFO",
    "logstash.shutdownwatcher" : "INFO",
    "org.logstash.Event" : "INFO",
    "slowlog.logstash.codecs.plain" : "TRACE",
    "slowlog.logstash.codecs.rubydebug" : "TRACE",
    "slowlog.logstash.filters.grok" : "TRACE",
    "slowlog.logstash.inputs.beats" : "TRACE",
    "slowlog.logstash.outputs.stdout" : "TRACE"
  }
}
--------------------------------------------------

===== Update logging levels

Prepend the name of the subsystem, module, or plugin with `logger.`. 

Here is an example using `outputs.elasticsearch`:

[source,js]
--------------------------------------------------
curl -XPUT 'localhost:9600/_node/logging?pretty' -H 'Content-Type: application/json' -d'
{
    "logger.logstash.outputs.elasticsearch" : "DEBUG"
}
'
--------------------------------------------------

While this setting is in effect, Logstash emits DEBUG-level logs for __all__ the Elasticsearch outputs
specified in your configuration. Please note this new setting is transient and will not survive a restart.

NOTE: If you want logging changes to persist after a restart, add them to `log4j2.properties` instead. 

===== Reset dynamic logging levels

To reset any logging levels that may have been dynamically changed via the logging API, send a `PUT` request to
`_node/logging/reset`. All logging levels will revert to the values specified in the `log4j2.properties` file.

[source,js]
--------------------------------------------------
curl -XPUT 'localhost:9600/_node/logging/reset?pretty'
--------------------------------------------------

==== Log file location

You can specify the log file location using `--path.logs` setting.

==== Slowlog

Slowlog for Logstash adds the ability to log when a specific event takes an abnormal amount of time to make its way
through the pipeline. Just like the normal application log, you can find slowlogs in your `--path.logs` directory.
Slowlog is configured in the `logstash.yml` settings file with the following options:

[source,yaml]
------------------------------
slowlog.threshold.warn (default: -1)
slowlog.threshold.info (default: -1)
slowlog.threshold.debug (default: -1)
slowlog.threshold.trace (default: -1)
------------------------------

Slowlog is disabled by default. The default threshold values are set to
`-1nanos` to represent an infinite threshold. No slowlog will be invoked. 

===== Enable slowlog

The `slowlog.threshold` fields use a time-value format which enables a wide
range of trigger intervals. You can specify ranges using the following time
units: `nanos` (nanoseconds), `micros` (microseconds), `ms` (milliseconds), `s`
(second), `m` (minute), `h` (hour), `d` (day).

Slowlog becomes more sensitive and logs more events as you raise the log level. 

Example:

[source,yaml]
------------------------------
slowlog.threshold.warn: 2s
slowlog.threshold.info: 1s
slowlog.threshold.debug: 500ms
slowlog.threshold.trace: 100ms
------------------------------

In this example:

* If the log level is set to `warn`, the log shows events that took longer than 2s to process.
* If the log level is set to `info`, the log shows events that took longer than 1s to process.
* If the log level is set to `debug`, the log shows events that took longer than 500ms to process.
* If the log level is set to `trace`, the log shows events that took longer than 100ms to process.

The logs include the full event and filter configuration that are responsible
for the slowness.
